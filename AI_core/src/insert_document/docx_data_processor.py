#!/usr/bin/env python3
"""
DOCX Data Processor
Xá»­ lÃ½ tÃ i liá»‡u DOCX tá»« nguá»“n Ä‘áº¿n embedding tá»± Ä‘á»™ng
- Load tÃ i liá»‡u DOCX
- LÃ m sáº¡ch vÃ  chuáº©n hÃ³a ná»™i dung  
- Chia nhá» thÃ nh chunks
- Táº¡o embedding qua API
- LÆ°u vÃ o vector database
"""

import os
import re
import uuid
import requests
import logging
import math
from collections import Counter
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain_community.document_loaders import Docx2txtLoader
from langchain.text_splitter import TokenTextSplitter

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BM25Encoder:
    """Simple BM25 encoder for sparse vector generation"""

    def __init__(self, k1: float = 1.2, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.vocabulary = {}
        self.doc_freqs = {}
        self.idf = {}
        self.corpus_size = 0
        self.avgdl = 0
        self.corpus_stats_ready = False

    def tokenize(self, text: str) -> List[str]:
        """Simple tokenization"""
        tokens = re.findall(r'\b\w+\b', text.lower())
        return tokens

    def build_corpus_statistics(self, documents: List[str]) -> None:
        """Build BM25 statistics from corpus"""
        try:
            self.corpus_size = len(documents)
            doc_freqs = {}
            total_doc_length = 0

            for document in documents:
                tokens = self.tokenize(document)
                total_doc_length += len(tokens)

                # Count unique terms in this document for document frequency
                unique_tokens = set(tokens)
                for token in unique_tokens:
                    doc_freqs[token] = doc_freqs.get(token, 0) + 1

            self.doc_freqs = doc_freqs
            self.avgdl = total_doc_length / self.corpus_size if self.corpus_size > 0 else 0

            # Calculate IDF for each term
            for token, freq in doc_freqs.items():
                # BM25 IDF formula: log((N - df + 0.5) / (df + 0.5))
                self.idf[token] = math.log((self.corpus_size - freq + 0.5) / (freq + 0.5))

                # Build vocabulary mapping
                if token not in self.vocabulary:
                    self.vocabulary[token] = len(self.vocabulary)

            self.corpus_stats_ready = True
            logger.info(f"ğŸ” BM25 corpus statistics built: {self.corpus_size} documents, {len(self.vocabulary)} unique terms")

        except Exception as e:
            logger.error(f"âŒ Failed to build BM25 corpus statistics: {e}")
            raise

    def encode(self, text: str, doc_length: Optional[int] = None) -> Dict[str, Any]:
        """Encode text to BM25 sparse vector in Qdrant format"""
        if not self.corpus_stats_ready:
            logger.warning("âš ï¸ BM25 corpus statistics not ready, returning empty sparse vector")
            return {"indices": [], "values": []}

        try:
            tokens = self.tokenize(text)
            token_counts = Counter(tokens)

            indices = []
            values = []

            # Use provided doc_length or calculate from tokens
            doc_len = doc_length if doc_length is not None else len(tokens)

            for token, tf in token_counts.items():
                if token in self.idf and token in self.vocabulary:
                    token_idx = self.vocabulary[token]
                    idf = self.idf[token]

                    # BM25 formula: IDF * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * |D| / avgdl))
                    numerator = idf * tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)
                    score = numerator / denominator

                    if score > 0:
                        indices.append(token_idx)
                        values.append(score)

            return {"indices": indices, "values": values}

        except Exception as e:
            logger.error(f"âŒ Failed to encode text: {e}")
            return {"indices": [], "values": []}

    def get_corpus_info(self) -> Dict[str, Any]:
        """Get corpus statistics info"""
        return {
            "corpus_size": self.corpus_size,
            "vocabulary_size": len(self.vocabulary),
            "average_doc_length": self.avgdl,
            "stats_ready": self.corpus_stats_ready
        }


class DocxDataProcessor:
    """Processor Ä‘á»ƒ xá»­ lÃ½ tÃ i liá»‡u DOCX tá»± Ä‘á»™ng"""
    
    def __init__(
        self,
        embed_service_url: str = "http://13.210.111.152:8005",
        database_service_url: str = "http://13.210.111.152:8002",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        enable_bm25: bool = True
    ):
        self.embed_service_url = embed_service_url
        self.database_service_url = database_service_url
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.enable_bm25 = enable_bm25

        # Text splitter cho chunking
        self.text_splitter = TokenTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

        # BM25 encoder for sparse vectors
        self.bm25_encoder = BM25Encoder() if enable_bm25 else None
        self.corpus_texts = []  # Store texts for BM25 corpus building

        # Stats tracking
        self.stats = {
            "files_processed": 0,
            "total_chunks": 0,
            "successful_embeddings": 0,
            "failed_embeddings": 0,
            "successful_upserts": 0,
            "failed_upserts": 0,
            "bm25_enabled": enable_bm25
        }
        
    def load_docx(self, file_path: str) -> str:
        """Load ná»™i dung tá»« file DOCX"""
        try:
            logger.info(f"ğŸ“„ Loading DOCX file: {file_path}")
            loader = Docx2txtLoader(file_path)
            documents = loader.load()
            
            if not documents:
                raise ValueError("KhÃ´ng thá»ƒ load ná»™i dung tá»« file DOCX")
                
            content = "\n".join([doc.page_content for doc in documents])
            logger.info(f"âœ… Loaded {len(content)} characters from DOCX")
            return content
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i load DOCX {file_path}: {e}")
            raise
    
    def clean_text(self, text: str) -> str:
        """LÃ m sáº¡ch vÃ  chuáº©n hÃ³a text"""
        try:
            # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  normalize whitespace
            text = re.sub(r'[ \t]+', ' ', text)  # Multiple spaces/tabs -> single space
            text = re.sub(r'\n+', '\n', text)  # Multiple newlines -> single newline
            text = re.sub(r'\r', '', text)  # Remove carriage returns
            text = text.strip()

            # Loáº¡i bá» cÃ¡c dÃ²ng trá»‘ng
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)

            logger.debug(f"ğŸ§¹ Cleaned text: {len(text)} -> {len(cleaned_text)} chars")
            return cleaned_text

        except Exception as e:
            logger.error(f"âŒ Lá»—i clean text: {e}")
            return text
    
    def extract_metadata(self, file_path: str) -> Dict[str, str]:
        """Extract metadata tá»« tÃªn file"""
        try:
            file_name = Path(file_path).stem
            parts = file_name.split()
            
            # Máº·c Ä‘á»‹nh metadata
            metadata = {
                "subject": "unknown",
                "week": "unknown",
                "title": file_name
            }
            
            # TÃ¬m pattern "Module X" hoáº·c "Week X"
            for part in parts:
                if part.lower().startswith('module'):
                    metadata["subject"] = part.lower()
                elif part.lower().startswith('week') or part.lower().startswith('w'):
                    metadata["week"] = part.lower()
                elif 's2' in part.lower() or 's1' in part.lower():
                    metadata["semester"] = part.lower()
            
            logger.debug(f"ğŸ“‹ Extracted metadata: {metadata}")
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i extract metadata: {e}")
            return {"subject": "unknown", "week": "unknown", "title": Path(file_path).stem}
    
    def chunk_text(self, text: str) -> List[str]:
        """Chia text thÃ nh chunks"""
        try:
            if not text.strip():
                return [""]  # Return empty string for empty input

            chunks = self.text_splitter.split_text(text)
            logger.info(f"âœ‚ï¸ Split into {len(chunks)} chunks")
            return chunks if chunks else [text]  # Ensure at least one chunk

        except Exception as e:
            logger.error(f"âŒ Lá»—i chunk text: {e}")
            return [text]  # Fallback: return original text as single chunk
    
    def embed_text(self, text: str) -> Optional[List[float]]:
        """Táº¡o embedding cho text qua API"""
        try:
            response = requests.post(
                f"{self.embed_service_url}/embed",
                json={"text": text},
                timeout=30
            )
            response.raise_for_status()
            
            embedding = response.json()["embedding"]
            self.stats["successful_embeddings"] += 1
            logger.debug(f"ğŸ”¢ Created embedding: {len(embedding)} dimensions")
            return embedding
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i embed text: {e}")
            self.stats["failed_embeddings"] += 1
            return None
    
    def create_sparse_vector(self, text: str) -> Optional[Dict[str, Any]]:
        """Táº¡o sparse vector tá»« text sá»­ dá»¥ng BM25"""
        if not self.enable_bm25 or not self.bm25_encoder:
            return None

        try:
            sparse_vector = self.bm25_encoder.encode(text)
            logger.debug(f"ğŸ” Created sparse vector: {len(sparse_vector.get('indices', []))} terms")
            return sparse_vector
        except Exception as e:
            logger.error(f"âŒ Lá»—i táº¡o sparse vector: {e}")
            return None

    def build_bm25_corpus(self, texts: List[str]):
        """Build BM25 corpus tá»« danh sÃ¡ch texts"""
        if not self.enable_bm25 or not self.bm25_encoder:
            return

        try:
            logger.info(f"ğŸ” Building BM25 corpus from {len(texts)} texts...")
            self.bm25_encoder.build_corpus_statistics(texts)
            logger.info("âœ… BM25 corpus built successfully")
        except Exception as e:
            logger.error(f"âŒ Lá»—i build BM25 corpus: {e}")

    def create_payload(self, chunk: str, metadata: Dict[str, str], chunk_id: int) -> Dict[str, Any]:
        """Táº¡o payload theo format chuáº©n vá»›i hybrid vectors"""
        payload = {
            "id": str(uuid.uuid4()),
            "vector": None,  # Dense vector - sáº½ Ä‘Æ°á»£c set sau khi embed
            "payload": {
                "content": chunk,
                "subject": metadata["subject"],
                "title": metadata["title"],
                "week": metadata["week"],
                "chunk_id": chunk_id,
                "file_path": metadata.get("file_path", "")
            }
        }

        # Add sparse vector if BM25 enabled
        if self.enable_bm25:
            sparse_vector = self.create_sparse_vector(chunk)
            if sparse_vector:
                payload["sparse_vector"] = sparse_vector

        return payload
    
    def upsert_document(self, payload: Dict[str, Any]) -> bool:
        """Upsert document vÃ o vector database vá»›i hybrid vectors"""
        try:
            # Prepare the point for Qdrant format
            point = {
                "id": payload["id"],
                "vector": {},
                "payload": payload["payload"]
            }

            # Add dense vector
            if payload.get("vector"):
                point["vector"]["dense_vector"] = payload["vector"]

            # Add sparse vector if available
            if payload.get("sparse_vector"):
                point["vector"]["bm25_sparse_vector"] = payload["sparse_vector"]

            request_data = {"points": [point]}

            response = requests.post(
                f"{self.database_service_url}/upsert",
                json=request_data,
                timeout=30
            )
            response.raise_for_status()

            success = response.json().get("success", False)
            if success:
                self.stats["successful_upserts"] += 1
                logger.debug(f"âœ… Upserted with {'hybrid' if payload.get('sparse_vector') else 'dense'} vectors")
            else:
                self.stats["failed_upserts"] += 1

            return success

        except Exception as e:
            logger.error(f"âŒ Lá»—i upsert: {e}")
            self.stats["failed_upserts"] += 1
            return False
    
    def process_file(self, file_path: str) -> Dict[str, Any]:
        """Xá»­ lÃ½ má»™t file DOCX hoÃ n chá»‰nh vá»›i hybrid vectors"""
        try:
            logger.info(f"ğŸš€ Báº¯t Ä‘áº§u xá»­ lÃ½ file: {file_path}")

            # 1. Load DOCX
            content = self.load_docx(file_path)

            # 2. Clean text
            cleaned_content = self.clean_text(content)

            # 3. Extract metadata
            metadata = self.extract_metadata(file_path)
            metadata["file_path"] = file_path

            # 4. Chunk text
            chunks = self.chunk_text(cleaned_content)

            # 5. Build BM25 corpus if enabled
            if self.enable_bm25:
                logger.info("ğŸ” Building BM25 corpus for sparse vectors...")
                self.build_bm25_corpus(chunks)

            # 6. Process tá»«ng chunk
            successful_chunks = 0
            failed_chunks = 0

            for i, chunk in enumerate(chunks):
                logger.info(f"ğŸ“„ Processing chunk {i+1}/{len(chunks)}")

                # Embed chunk (dense vector)
                embedding = self.embed_text(chunk)
                if embedding is None:
                    failed_chunks += 1
                    continue

                # Táº¡o payload vá»›i hybrid vectors
                payload = self.create_payload(chunk, metadata, i)
                payload["vector"] = embedding

                # Upsert vÃ o database
                success = self.upsert_document(payload)
                if success:
                    successful_chunks += 1
                    vector_type = "hybrid" if payload.get("sparse_vector") else "dense"
                    logger.debug(f"âœ… Chunk {i+1} processed successfully ({vector_type})")
                else:
                    failed_chunks += 1
                    logger.warning(f"âŒ Chunk {i+1} failed to upsert")

            # Update stats
            self.stats["files_processed"] += 1
            self.stats["total_chunks"] += len(chunks)

            result = {
                "success": True,
                "file_path": file_path,
                "total_chunks": len(chunks),
                "successful_chunks": successful_chunks,
                "failed_chunks": failed_chunks,
                "metadata": metadata,
                "bm25_enabled": self.enable_bm25
            }

            logger.info(f"âœ… File processed: {successful_chunks}/{len(chunks)} chunks successful")
            if self.enable_bm25:
                logger.info(f"ğŸ” BM25 sparse vectors enabled for hybrid search")
            return result

        except Exception as e:
            logger.error(f"âŒ Lá»—i xá»­ lÃ½ file {file_path}: {e}")
            return {
                "success": False,
                "file_path": file_path,
                "error": str(e)
            }
    
    def process_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """Xá»­ lÃ½ táº¥t cáº£ file DOCX trong thÆ° má»¥c"""
        try:
            directory = Path(directory_path)
            docx_files = list(directory.glob("*.docx"))
            
            if not docx_files:
                logger.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file DOCX nÃ o trong {directory_path}")
                return []
            
            logger.info(f"ğŸ“ TÃ¬m tháº¥y {len(docx_files)} file DOCX")
            
            results = []
            for file_path in docx_files:
                result = self.process_file(str(file_path))
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i xá»­ lÃ½ thÆ° má»¥c {directory_path}: {e}")
            return []
    
    def get_stats(self) -> Dict[str, int]:
        """Láº¥y thá»‘ng kÃª xá»­ lÃ½"""
        return self.stats.copy()
    
    def reset_stats(self):
        """Reset thá»‘ng kÃª (except bm25_enabled)"""
        for key in self.stats:
            if key != "bm25_enabled":
                self.stats[key] = 0


def main():
    """Main function Ä‘á»ƒ demo basic usage vá»›i hybrid vectors"""
    print("ğŸš€ DocxDataProcessor Demo (Hybrid Vectors)")
    print("="*50)

    # Initialize processor vá»›i BM25 enabled
    processor = DocxDataProcessor(
        embed_service_url="http://localhost:8005",
        database_service_url="http://localhost:8002",
        chunk_size=300,
        chunk_overlap=50,
        enable_bm25=True  # Enable hybrid search
    )

    print(f"ğŸ” BM25 Sparse Vectors: {'âœ… Enabled' if processor.enable_bm25 else 'âŒ Disabled'}")

    # Check if test file exists
    test_file = "./data/Module 6 S2 2025.docx"
    if not Path(test_file).exists():
        print(f"âš ï¸ Test file not found: {test_file}")
        print("ğŸ’¡ Please add a DOCX file to the data directory")
        return

    try:
        print(f"ğŸ“„ Processing: {test_file}")
        result = processor.process_file(test_file)

        if result["success"]:
            print(f"âœ… Success!")
            print(f"ğŸ“Š Chunks: {result['successful_chunks']}/{result['total_chunks']}")
            print(f"ï¿½ BM25 Enabled: {result.get('bm25_enabled', False)}")
            print(f"ï¿½ğŸ“‹ Metadata: {result['metadata']}")
        else:
            print(f"âŒ Failed: {result['error']}")

        # Show stats
        stats = processor.get_stats()
        print(f"\nğŸ“ˆ Stats:")
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # Show BM25 corpus info if available
        if processor.enable_bm25 and processor.bm25_encoder and processor.bm25_encoder.corpus_stats_ready:
            corpus_info = processor.bm25_encoder.get_corpus_info()
            print(f"\nğŸ” BM25 Corpus Info:")
            for key, value in corpus_info.items():
                print(f"  {key}: {value}")

    except Exception as e:
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()
