# DOCX Document Processing Pipeline

H·ªá th·ªëng x·ª≠ l√Ω t√†i li·ªáu DOCX t·ª± ƒë·ªông t·ª´ ƒë·ªçc file ‚Üí l√†m s·∫°ch ‚Üí chia nh·ªè ‚Üí embedding ‚Üí l∆∞u tr·ªØ v·ªõi **Hybrid Search** (Dense + Sparse Vectors).

## üéØ T√≠nh nƒÉng ch√≠nh

- **ƒê·ªçc DOCX**: S·ª≠ d·ª•ng LangChain Docx2txtLoader ƒë·ªÉ extract text
- **L√†m s·∫°ch d·ªØ li·ªáu**: Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, normalize whitespace
- **Chia nh·ªè (Chunking)**: Chia document th√†nh chunks v·ªõi TokenTextSplitter
- **Metadata extraction**: T·ª± ƒë·ªông extract subject, week t·ª´ t√™n file
- **Dense Embedding**: T√≠ch h·ª£p v·ªõi embedding service ƒë·ªÉ t·∫°o dense vector (512-dim)
- **Sparse Vectors**: BM25 encoder ƒë·ªÉ t·∫°o sparse vectors cho hybrid search
- **Hybrid Storage**: L∆∞u c·∫£ dense v√† sparse vectors v√†o Qdrant database
- **Batch processing**: X·ª≠ l√Ω nhi·ªÅu files c√πng l√∫c
- **Error handling**: X·ª≠ l√Ω l·ªói graceful v√† logging chi ti·∫øt

## üìÅ C·∫•u tr√∫c files

```
AI_core/src/insert_document/
‚îú‚îÄ‚îÄ docx_data_processor.py    # Main processor class
‚îú‚îÄ‚îÄ test_docx_processor.py    # Unit tests
‚îú‚îÄ‚îÄ example_usage.py          # Usage examples
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îú‚îÄ‚îÄ README.md                # Documentation
‚îî‚îÄ‚îÄ data/                    # Test data directory
    ‚îî‚îÄ‚îÄ Module 6 S2 2025.docx
```

## üöÄ Quick Start

### 1. C√†i ƒë·∫∑t dependencies

```bash
cd AI_core/src/insert_document
pip install -r requirements.txt
```

### 2. Kh·ªüi ƒë·ªông services

```bash
# T·ª´ root directory
docker-compose up embedding vectordb
```

### 3. Ch·∫°y processor

```python
from docx_data_processor import DocxDataProcessor

# Initialize processor v·ªõi hybrid search
processor = DocxDataProcessor(
    embed_service_url="http://localhost:8005",
    database_service_url="http://localhost:8002",
    enable_bm25=True  # Enable BM25 sparse vectors
)

# Process m·ªôt file v·ªõi hybrid vectors
result = processor.process_file("./data/Module 6 S2 2025.docx")
print(f"Processed {result['successful_chunks']}/{result['total_chunks']} chunks")
print(f"BM25 enabled: {result['bm25_enabled']}")

# Process c·∫£ th∆∞ m·ª•c
results = processor.process_directory("./data")
print(f"Processed {len(results)} files")
```

### 4. Ch·∫°y tests

```bash
python3 test_docx_processor.py
```

### 5. Xem examples

```bash
python3 example_usage.py
```

## üìñ API Reference

### DocxDataProcessor Class

#### Constructor

```python
DocxDataProcessor(
    embed_service_url="http://localhost:8005",
    database_service_url="http://localhost:8002",
    chunk_size=500,
    chunk_overlap=50,
    enable_bm25=True  # Enable hybrid search
)
```

#### Methods

- `process_file(file_path: str) -> Dict[str, Any]`: X·ª≠ l√Ω m·ªôt file DOCX
- `process_directory(directory_path: str) -> List[Dict[str, Any]]`: X·ª≠ l√Ω t·∫•t c·∫£ file DOCX trong th∆∞ m·ª•c
- `load_docx(file_path: str) -> str`: Load n·ªôi dung t·ª´ file DOCX
- `clean_text(text: str) -> str`: L√†m s·∫°ch text
- `chunk_text(text: str) -> List[str]`: Chia text th√†nh chunks
- `extract_metadata(file_path: str) -> Dict[str, str]`: Extract metadata t·ª´ t√™n file
- `embed_text(text: str) -> Optional[List[float]]`: T·∫°o embedding
- `upsert_document(payload: Dict[str, Any]) -> bool`: L∆∞u v√†o database
- `get_stats() -> Dict[str, int]`: L·∫•y th·ªëng k√™
- `reset_stats()`: Reset th·ªëng k√™

## üîß Configuration

### Environment Variables

```bash
# Service URLs
EMBED_SERVICE_URL=http://localhost:8005
DATABASE_SERVICE_URL=http://localhost:8002

# Processing settings
CHUNK_SIZE=500
CHUNK_OVERLAP=50
```

### Payload Format (Hybrid Vectors)

```json
{
  "id": "uuid-string",
  "vector": {
    "dense_vector": [0.1, 0.2, ...],  // 512-dim embedding
    "bm25_sparse_vector": {
      "indices": [1, 5, 10, ...],     // Term indices
      "values": [0.8, 0.6, 0.4, ...]  // BM25 scores
    }
  },
  "payload": {
    "content": "chunk text content",
    "subject": "module",
    "title": "Module 6 S2 2025",
    "week": "week06",
    "chunk_id": 0,
    "file_path": "/path/to/file.docx"
  }
}
```

## üß™ Testing

### Unit Tests

```bash
# Ch·∫°y t·∫•t c·∫£ tests
python3 test_docx_processor.py

# Test v·ªõi coverage
python3 -m coverage run test_docx_processor.py
python3 -m coverage report
```

### Integration Tests

```bash
# Test v·ªõi real file
python3 example_usage.py
```

## üìä Performance

- **Processing speed**: ~10-15 chunks/second
- **Memory usage**: ~50MB cho file 20MB
- **Chunk size**: 300-500 tokens optimal
- **Batch size**: 1-5 files optimal

## üîç Troubleshooting

### Common Issues

1. **Service connection errors**
   ```bash
   # Check services
   curl http://localhost:8005/health
   curl http://localhost:8002/health
   ```

2. **Memory issues v·ªõi large files**
   ```python
   # Reduce chunk size
   processor = DocxDataProcessor(chunk_size=300)
   ```

3. **Encoding issues**
   - ƒê·∫£m b·∫£o file DOCX kh√¥ng b·ªã corrupt
   - Ki·ªÉm tra encoding UTF-8

### Debug Mode

```python
import logging
logging.basicConfig(level=logging.DEBUG)

processor = DocxDataProcessor()
result = processor.process_file("file.docx")
```

## üöÄ Production Usage

### Batch Processing

```python
import os
from pathlib import Path

processor = DocxDataProcessor()

# Process t·∫•t c·∫£ files trong th∆∞ m·ª•c
for root, dirs, files in os.walk("documents/"):
    for file in files:
        if file.endswith('.docx'):
            file_path = os.path.join(root, file)
            result = processor.process_file(file_path)
            print(f"Processed: {file} - {result['successful_chunks']} chunks")
```

### Error Handling

```python
try:
    result = processor.process_file("file.docx")
    if result["success"]:
        print(f"‚úÖ Success: {result['successful_chunks']} chunks")
    else:
        print(f"‚ùå Failed: {result['error']}")
except Exception as e:
    print(f"‚ùå Exception: {e}")
```

## üìà Monitoring

```python
# Check stats
stats = processor.get_stats()
print(f"Files processed: {stats['files_processed']}")
print(f"Success rate: {stats['successful_upserts']}/{stats['total_chunks']}")
```

## ü§ù Contributing

1. Fork repository
2. T·∫°o feature branch
3. Th√™m tests cho new features
4. Ch·∫°y t·∫•t c·∫£ tests
5. Submit pull request

## üìÑ License

MIT License - see LICENSE file for details.
