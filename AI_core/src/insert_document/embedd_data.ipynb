{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29e6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad75c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMMBED_SERVICE_URL = \"http://localhost:8005\"\n",
    "DATABASE_SERVICE_URL = \"http://localhost:8002\"\n",
    "DATA_FILE = './data/D·ªçn d·∫πp bu·ªìng ph√≤ng_structured.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295e6417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Gi·ªõi thi·ªáu chung\\nD·ªãch v·ª• D·ªçn d·∫πp Bu·ªìng ph√≤ng c·ªßa bTaskee- Gi·∫£i ph√°p d·ªçn d·∫πp nhanh ch√≥ng v√† ti·ªán l·ª£i cho ch·ªß c√°c h·ªá th·ªëng kh√°ch s·∫°n, homestay, cƒÉn h·ªô d·ªãch v·ª•, villa, nh√† nguy√™n cƒÉn. ƒêƒÉng l·ªãch v·ªõi 60 gi√¢y v√† ch·ªâ c·∫ßn t·ª´ 5 ph√∫t ƒë·ªÉ c√≥ ng∆∞·ªùi gi√∫p vi·ªác theo gi·ªù nh·∫≠n vi·ªác. C·∫ßn l√∫c n√†o ƒë·∫∑t l·ªãch l√∫c ƒë√≥ n√™n c√≥ th·ªÉ gi·∫£m chi ph√≠ thu√™ v√† ƒë√†o t·∫°o nh√¢n s·ª± c·ªë ƒë·ªãnh. C√°c nh√¢n vi√™n bu·ªìng ph√≤ng c·ªßa bTaskee ƒë∆∞·ª£c s√†ng l·ªçc k·ªπ c√†ng, ph·∫£i c√≥ √≠t nh·∫•t 1 nƒÉm kinh nghi·ªám l√†m vi·ªác t·∫°i kh√°ch s·∫°n 3 sao tr·ªü l√™n. Gi√° d·ªãch v·ª• s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã chi ti·∫øt tr√™n ·ª©ng d·ª•ng. Ng∆∞·ªùi d√πng kh√¥ng ph·∫£i tr·∫£ th√™m b·∫•t k√¨ ph√≠ ph√°t sinh n√†o.',\n",
       " '\\n- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\\nH∆∞·ªõng d·∫´n ƒë·∫∑t d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng\\nB∆∞·ªõc 1: Ch·ªçn ƒë·ªãa ch·ªâ c·∫ßn ƒë·∫∑t l·ªãch\\nB∆∞·ªõc 2: Ch·ªçn lo·∫°i h√¨nh nh√† v√† lo·∫°i h√¨nh ph√≤ng.\\nB∆∞·ªõc 3: Ch·ªçn s·ªë l∆∞·ª£ng ph√≤ng c·∫ßn d·ªçn d·∫πp\\nB∆∞·ªõc 4: Ch·ªçn c√°c d·ªãch v·ª• th√™m( d·ªçn s·∫£nh, setup theo h√¨nh,..)\\nB∆∞·ªõc 5: Ch·ªçn ng√†y gi·ªù l√†m vi·ªác v√† ghi ch√∫ c√¥ng vi·ªác\\nB∆∞·ªõc 6: X√°c nh·∫≠n v√† thanh to√°n\\nB∆∞·ªõc 7: ƒê·∫∑t l·ªãch',\n",
       " '\\n- L∆∞u √Ω\\n- ƒê∆∞·ª£c ch·ªçn c√πng l√∫c nhi·ªÅu lo·∫°i ph√≤ng, nh∆∞ng t·ªëi ƒëa l√† 10 ph√≤ng cho m·ªói lo·∫°i ph√≤ng.\\n- M·ªói ph√≤ng s·∫Ω c√≥ th·ªùi l∆∞·ª£ng d·ªçn d·∫πp kh√°c nhau, nh∆∞ng t·ªïng th·ªùi l∆∞·ª£ng c√¥ng vi·ªác t·ªëi ƒëa l√† 6 gi·ªù.\\n- Ph·ª• thu th√™m ph√≠ h√≥a ch·∫•t d·ªçn d·∫πp:\\n- 30.000vnd n·∫øu th·ªùi l∆∞·ª£ng l√†m vi·ªác >= 3 gi·ªù (3h v√† 4h)\\n- 50.000vnd n·∫øu th·ªùi l∆∞·ª£ng l√†m vi·ªác >= 5 gi·ªù (5h v√† 6h)\\n- ƒê·ªëi v·ªõi Khung gi·ªù cao ƒëi·ªÉm (tr∆∞·ªõc 8h00 v√† sau 19h00) v√† Th·ª© 7, Ch·ªß Nh·∫≠t gi√° d·ªãch v·ª• tƒÉng 20%.\\n- Gi√° mang t√≠nh ch·∫•t tham kh·∫£o ·ªü th·ªùi ƒëi·ªÉm hi·ªán t·∫°i. Gi√° d·ªãch v·ª• c√≥ th·ªÉ t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh t√πy v√†o khu v·ª±c, gi·ªù cao ƒëi·ªÉm, l·ªÖ t·∫øt.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DATA_FILE, 'r', encoding='utf-8') as file:\n",
    "    data = file.read().split(\"\\n\\n\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6640b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingPipeline:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.payloads = []\n",
    "    \n",
    "    def embed_text(self, text: str) -> list:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                EMMBED_SERVICE_URL + \"/embed\", \n",
    "                json={\"text\": text},\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"embedding\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói embed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def upsert_document(self, payload: dict) -> bool:\n",
    "        try:\n",
    "            # ‚úÖ Wrap payload trong \"points\" array\n",
    "            request_data = {\n",
    "                \"points\": [payload]  # API mong ƒë·ª£i array of points\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                DATABASE_SERVICE_URL + \"/upsert\", \n",
    "                json=request_data,  # G·ª≠i request_data thay v√¨ payload\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"success\", False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói upsert: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process(self, data):\n",
    "        print(f\"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(data)} documents...\")\n",
    "        \n",
    "        # B∆∞·ªõc 1: T·∫°o embeddings v√† payloads\n",
    "        for i, document in enumerate(data):\n",
    "            try:\n",
    "                print(f\"üìÑ Processing {i+1}/{len(data)}: {str(document)[:50]}...\")\n",
    "                \n",
    "                embedding = self.embed_text(str(document))\n",
    "                \n",
    "                payload = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"vector\": embedding,\n",
    "                    \"payload\": {\n",
    "                        \"text\": str(document),\n",
    "                        \"user_id\": \"test_user\",\n",
    "                        \"title\": \"D·ªçn d·∫πp bu·ªìng ph√≤ng_structured.txt\",\n",
    "                        \"file_id\": str(uuid.uuid4()),\n",
    "                        \"source\": \"D·ªçn d·∫πp bu·ªìng ph√≤ng_structured.txt\",\n",
    "                        \"page\": i\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                self.payloads.append(payload)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói x·ª≠ l√Ω document {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # B∆∞·ªõc 2: Upsert t·ª´ng payload\n",
    "        print(f\"\\nüíæ B·∫Øt ƒë·∫ßu upsert {len(self.payloads)} payloads...\")\n",
    "        success_count = 0\n",
    "        \n",
    "        for i, payload in enumerate(self.payloads):\n",
    "            try:\n",
    "                success = self.upsert_document(payload)\n",
    "                if success:\n",
    "                    success_count += 1\n",
    "                    print(f\"‚úÖ Upsert {i+1}/{len(self.payloads)} th√†nh c√¥ng\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Upsert {i+1}/{len(self.payloads)} th·∫•t b·∫°i\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói upsert {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüìä Ho√†n th√†nh: {success_count}/{len(self.payloads)} documents th√†nh c√¥ng\")\n",
    "        return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21390657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 3 documents...\n",
      "üìÑ Processing 1/3: - Gi·ªõi thi·ªáu chung\n",
      "D·ªãch v·ª• D·ªçn d·∫πp Bu·ªìng ph√≤ng c·ªßa...\n",
      "üìÑ Processing 2/3: \n",
      "- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
      "H∆∞·ªõng d·∫´n ƒë·∫∑t d·ªãch v·ª• d·ªçn d·∫πp...\n",
      "üìÑ Processing 3/3: \n",
      "- L∆∞u √Ω\n",
      "- ƒê∆∞·ª£c ch·ªçn c√πng l√∫c nhi·ªÅu lo·∫°i ph√≤ng, nh...\n",
      "\n",
      "üíæ B·∫Øt ƒë·∫ßu upsert 3 payloads...\n",
      "‚úÖ Upsert 1/3 th√†nh c√¥ng\n",
      "‚úÖ Upsert 2/3 th√†nh c√¥ng\n",
      "‚úÖ Upsert 3/3 th√†nh c√¥ng\n",
      "\n",
      "üìä Ho√†n th√†nh: 3/3 documents th√†nh c√¥ng\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ ƒê√∫ng\n",
    "pipeline = ProcessingPipeline()\n",
    "result = pipeline.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b16331a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching: 'Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng'\n",
      "‚úÖ Query embedded: 512 dimensions\n",
      "‚úÖ Found 3 results\n",
      "\n",
      "üìä K·∫æT QU·∫¢ SEARCH (3 documents):\n",
      "============================================================\n",
      "\n",
      "üî∏ K·∫øt qu·∫£ 1:\n",
      "   üìä Score: 0.665\n",
      "   üìÑ Text: \n",
      "- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
      "H∆∞·ªõng d·∫´n ƒë·∫∑t d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng\n",
      "B∆∞·ªõc 1: Ch·ªçn ƒë·ªãa ch·ªâ c·∫ßn ƒë·∫∑t l·ªãch\n",
      "B∆∞·ªõc 2: Ch·ªçn lo·∫°i h√¨nh nh√† v√† lo·∫°i h√¨nh ph√≤ng.\n",
      "B∆∞·ªõc 3: Ch·ªçn s·ªë l∆∞·ª£ng ph√≤ng c·∫ßn d·ªçn d·∫πp\n",
      "B∆∞·ªõc 4: Ch·ªçn c√°c...\n",
      "   üë§ User: test_user\n",
      "   üìÅ File: e86f28a6-de5f-4992-9321-f6cad218cf16\n",
      "   üìñ Page: 1\n",
      "\n",
      "üî∏ K·∫øt qu·∫£ 2:\n",
      "   üìä Score: 0.623\n",
      "   üìÑ Text: \n",
      "- L∆∞u √Ω\n",
      "- ƒê∆∞·ª£c ch·ªçn c√πng l√∫c nhi·ªÅu lo·∫°i ph√≤ng, nh∆∞ng t·ªëi ƒëa l√† 10 ph√≤ng cho m·ªói lo·∫°i ph√≤ng.\n",
      "- M·ªói ph√≤ng s·∫Ω c√≥ th·ªùi l∆∞·ª£ng d·ªçn d·∫πp kh√°c nhau, nh∆∞ng t·ªïng th·ªùi l∆∞·ª£ng c√¥ng vi·ªác t·ªëi ƒëa l√† 6 gi·ªù.\n",
      "- Ph·ª• thu ...\n",
      "   üë§ User: test_user\n",
      "   üìÅ File: d10766e1-2ae9-4a4c-bddb-d96cf2bc94e5\n",
      "   üìñ Page: 2\n",
      "\n",
      "üî∏ K·∫øt qu·∫£ 3:\n",
      "   üìä Score: 0.618\n",
      "   üìÑ Text: - Gi·ªõi thi·ªáu chung\n",
      "D·ªãch v·ª• D·ªçn d·∫πp Bu·ªìng ph√≤ng c·ªßa bTaskee- Gi·∫£i ph√°p d·ªçn d·∫πp nhanh ch√≥ng v√† ti·ªán l·ª£i cho ch·ªß c√°c h·ªá th·ªëng kh√°ch s·∫°n, homestay, cƒÉn h·ªô d·ªãch v·ª•, villa, nh√† nguy√™n cƒÉn. ƒêƒÉng l·ªãch v·ªõi 60 ...\n",
      "   üë§ User: test_user\n",
      "   üìÅ File: 9d93dc46-fc85-4802-9f54-75c14d09d58d\n",
      "   üìñ Page: 0\n"
     ]
    }
   ],
   "source": [
    "class SearchPipeline:\n",
    "    def __init__(self):\n",
    "        self.embed_url = EMMBED_SERVICE_URL\n",
    "        self.db_url = DATABASE_SERVICE_URL\n",
    "    \n",
    "    def embed_query(self, text: str) -> list:\n",
    "        \"\"\"Convert text to embedding vector\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.embed_url}/embed\", \n",
    "            json={\"text\": text},\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"embedding\"]\n",
    "    \n",
    "    def vector_search(self, query_vector: list, limit: int = 5, score_threshold: float = 0.7):\n",
    "        \"\"\"Search using vector\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.db_url}/search\", \n",
    "            json={\n",
    "                \"query_vector\": query_vector,\n",
    "                \"limit\": limit,\n",
    "                \"score_threshold\": score_threshold\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def search(self, query_text: str, limit: int = 5, score_threshold: float = 0.7):\n",
    "        \"\"\"End-to-end search from text query\"\"\"\n",
    "        print(f\"üîç Searching: '{query_text}'\")\n",
    "        \n",
    "        # Embed query\n",
    "        query_vector = self.embed_query(query_text)\n",
    "        print(f\"‚úÖ Query embedded: {len(query_vector)} dimensions\")\n",
    "        \n",
    "        # Search\n",
    "        results = self.vector_search(query_vector, limit, score_threshold)\n",
    "        print(f\"‚úÖ Found {results['total_found']} results\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, results):\n",
    "        \"\"\"Display search results nicely\"\"\"\n",
    "        if not results or not results.get(\"results\"):\n",
    "            print(\"‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìä K·∫æT QU·∫¢ SEARCH ({results['total_found']} documents):\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(results[\"results\"]):\n",
    "            print(f\"\\nüî∏ K·∫øt qu·∫£ {i+1}:\")\n",
    "            print(f\"   üìä Score: {result['score']:.3f}\")\n",
    "            print(f\"   üìÑ Text: {result['payload']['text'][:200]}...\")\n",
    "            print(f\"   üë§ User: {result['payload'].get('user_id', 'N/A')}\")\n",
    "            print(f\"   üìÅ File: {result['payload'].get('file_id', 'N/A')}\")\n",
    "            print(f\"   üìñ Page: {result['payload'].get('page', 'N/A')}\")\n",
    "\n",
    "# S·ª≠ d·ª•ng\n",
    "searcher = SearchPipeline()\n",
    "\n",
    "# Test search\n",
    "search_query = \"Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng\"\n",
    "results = searcher.search(search_query, limit=3, score_threshold=0.5)\n",
    "searcher.display_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ae1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "import math\n",
    "\n",
    "class BM25Encoder:\n",
    "    def __init__(self, k1=1.2, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.vocabulary = {}\n",
    "        self.doc_freqs = {}\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        self.avgdl = 0\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        # Convert to lowercase and split by non-alphanumeric characters\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, corpus: List[str]):\n",
    "        \"\"\"Fit BM25 on corpus\"\"\"\n",
    "        nd = len(corpus)\n",
    "        doc_freqs = {}\n",
    "        \n",
    "        for document in corpus:\n",
    "            tokens = self.tokenize(document)\n",
    "            self.doc_len.append(len(tokens))\n",
    "            \n",
    "            # Count unique tokens in document\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                doc_freqs[token] = doc_freqs.get(token, 0) + 1\n",
    "        \n",
    "        self.doc_freqs = doc_freqs\n",
    "        self.avgdl = sum(self.doc_len) / len(self.doc_len)\n",
    "        \n",
    "        # Calculate IDF\n",
    "        for token, freq in doc_freqs.items():\n",
    "            self.idf[token] = math.log((nd - freq + 0.5) / (freq + 0.5))\n",
    "    \n",
    "    def encode(self, text: str) -> Dict[int, float]:\n",
    "        \"\"\"Encode text to sparse vector\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        sparse_vector = {}\n",
    "        \n",
    "        for token, count in token_counts.items():\n",
    "            if token in self.idf:\n",
    "                # Get token index (create vocabulary on the fly)\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "                \n",
    "                token_idx = self.vocabulary[token]\n",
    "                \n",
    "                # BM25 score calculation\n",
    "                idf = self.idf[token]\n",
    "                tf = count\n",
    "                doc_len = len(tokens)\n",
    "                \n",
    "                score = idf * (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl))\n",
    "                \n",
    "                if score > 0:\n",
    "                    sparse_vector[token_idx] = score\n",
    "        \n",
    "        return sparse_vector\n",
    "\n",
    "# Global BM25 encoder\n",
    "bm25_encoder = BM25Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34eff12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 3 documents v·ªõi Hybrid Search...\n",
      "üîß Fitting BM25 on 3 documents...\n",
      "‚úÖ BM25 fitted successfully\n",
      "üìÑ Processing 1/3: - Gi·ªõi thi·ªáu chung\n",
      "D·ªãch v·ª• D·ªçn d·∫πp Bu·ªìng ph√≤ng c·ªßa...\n",
      "üìÑ Processing 2/3: \n",
      "- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
      "H∆∞·ªõng d·∫´n ƒë·∫∑t d·ªãch v·ª• d·ªçn d·∫πp...\n",
      "üìÑ Processing 3/3: \n",
      "- L∆∞u √Ω\n",
      "- ƒê∆∞·ª£c ch·ªçn c√πng l√∫c nhi·ªÅu lo·∫°i ph√≤ng, nh...\n",
      "\n",
      "üíæ B·∫Øt ƒë·∫ßu upsert 3 payloads...\n",
      "üîÑ Upsert batch 1: 3 documents...\n",
      "‚úÖ Batch 1 th√†nh c√¥ng (3 documents)\n",
      "\n",
      "üìä Ho√†n th√†nh: 3/3 documents th√†nh c√¥ng\n"
     ]
    }
   ],
   "source": [
    "class HybridProcessingPipeline:\n",
    "    def __init__(self, batch_size=5):\n",
    "        self.documents = []\n",
    "        self.payloads = []\n",
    "        self.batch_size = batch_size\n",
    "        self.bm25_encoder = BM25Encoder()\n",
    "        self.corpus_fitted = False\n",
    "    \n",
    "    def embed_text(self, text: str) -> list:\n",
    "        \"\"\"Get dense embedding\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                EMMBED_SERVICE_URL + \"/embed\", \n",
    "                json={\"text\": text},\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"embedding\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói embed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_sparse_vector(self, text: str) -> Dict[int, float]:\n",
    "        \"\"\"Create BM25 sparse vector\"\"\"\n",
    "        if not self.corpus_fitted:\n",
    "            print(\"‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit tr√™n corpus\")\n",
    "            return {}\n",
    "        \n",
    "        return self.bm25_encoder.encode(text)\n",
    "    \n",
    "    def fit_bm25(self, corpus: List[str]):\n",
    "        \"\"\"Fit BM25 on the corpus\"\"\"\n",
    "        print(f\"üîß Fitting BM25 on {len(corpus)} documents...\")\n",
    "        self.bm25_encoder.fit(corpus)\n",
    "        self.corpus_fitted = True\n",
    "        print(\"‚úÖ BM25 fitted successfully\")\n",
    "    \n",
    "    def upsert_batch(self, payloads_batch: list) -> bool:\n",
    "        \"\"\"Upsert batch v·ªõi hybrid vectors\"\"\"\n",
    "        try:\n",
    "            request_data = {\"points\": payloads_batch}\n",
    "            \n",
    "            response = requests.post(\n",
    "                DATABASE_SERVICE_URL + \"/upsert\", \n",
    "                json=request_data,\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"success\", False)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói upsert batch: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process(self, data):\n",
    "        print(f\"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(data)} documents v·ªõi Hybrid Search...\")\n",
    "        \n",
    "        # B∆∞·ªõc 1: Fit BM25 tr√™n to√†n b·ªô corpus\n",
    "        corpus = [str(doc) for doc in data]\n",
    "        self.fit_bm25(corpus)\n",
    "        \n",
    "        # B∆∞·ªõc 2: T·∫°o embeddings v√† sparse vectors\n",
    "        for i, document in enumerate(data):\n",
    "            try:\n",
    "                print(f\"üìÑ Processing {i+1}/{len(data)}: {str(document)[:50]}...\")\n",
    "                \n",
    "                # Dense embedding\n",
    "                dense_vector = self.embed_text(str(document))\n",
    "                \n",
    "                # Sparse vector (BM25)\n",
    "                sparse_vector = self.create_sparse_vector(str(document))\n",
    "                \n",
    "                # T·∫°o payload v·ªõi c·∫£ 2 lo·∫°i vector\n",
    "                payload = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"vector\": {\n",
    "                        \"dense_vector\": dense_vector,\n",
    "                        \"bm25_sparse_vector\": {\n",
    "                            \"indices\": list(sparse_vector.keys()),\n",
    "                            \"values\": list(sparse_vector.values())\n",
    "                        }\n",
    "                    },\n",
    "                    \"payload\": {\n",
    "                        \"text\": str(document),\n",
    "                        \"user_id\": \"test_user\",\n",
    "                        \"title\": \"Test Title\",\n",
    "                        \"file_id\": \"test_file_123\",\n",
    "                        \"source\": str(uuid.uuid4()),\n",
    "                        \"page\": 1\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                self.payloads.append(payload)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói x·ª≠ l√Ω document {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # B∆∞·ªõc 3: Upsert theo batch\n",
    "        print(f\"\\nüíæ B·∫Øt ƒë·∫ßu upsert {len(self.payloads)} payloads...\")\n",
    "        success_count = 0\n",
    "        \n",
    "        for i in range(0, len(self.payloads), self.batch_size):\n",
    "            batch = self.payloads[i:i + self.batch_size]\n",
    "            batch_num = i // self.batch_size + 1\n",
    "            \n",
    "            try:\n",
    "                print(f\"üîÑ Upsert batch {batch_num}: {len(batch)} documents...\")\n",
    "                success = self.upsert_batch(batch)\n",
    "                \n",
    "                if success:\n",
    "                    success_count += len(batch)\n",
    "                    print(f\"‚úÖ Batch {batch_num} th√†nh c√¥ng ({len(batch)} documents)\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Batch {batch_num} th·∫•t b·∫°i\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå L·ªói upsert batch {batch_num}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüìä Ho√†n th√†nh: {success_count}/{len(self.payloads)} documents th√†nh c√¥ng\")\n",
    "        return success_count\n",
    "\n",
    "# S·ª≠ d·ª•ng\n",
    "hybrid_pipeline = HybridProcessingPipeline()\n",
    "result = hybrid_pipeline.process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e164bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç Hybrid Search: 'Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng'\n",
      "   üìä Dense weight: 0.7, Sparse weight: 0.3\n",
      "‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit. Ch·ªâ s·ª≠ d·ª•ng dense search.\n",
      "‚ùå L·ªói hybrid search: 422 Client Error: Unprocessable Entity for url: http://localhost:8002/search\n",
      "üîÑ Fallback to dense search...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç Hybrid Search: 'quy tr√¨nh d·ªçn d·∫πp'\n",
      "   üìä Dense weight: 0.7, Sparse weight: 0.3\n",
      "‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit. Ch·ªâ s·ª≠ d·ª•ng dense search.\n",
      "‚ùå L·ªói hybrid search: 422 Client Error: Unprocessable Entity for url: http://localhost:8002/search\n",
      "üîÑ Fallback to dense search...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç Hybrid Search: 'thi·∫øt b·ªã an to√†n'\n",
      "   üìä Dense weight: 0.7, Sparse weight: 0.3\n",
      "‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit. Ch·ªâ s·ª≠ d·ª•ng dense search.\n",
      "‚ùå L·ªói hybrid search: 422 Client Error: Unprocessable Entity for url: http://localhost:8002/search\n",
      "üîÑ Fallback to dense search...\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç Hybrid Search: 'h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng'\n",
      "   üìä Dense weight: 0.7, Sparse weight: 0.3\n",
      "‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit. Ch·ªâ s·ª≠ d·ª•ng dense search.\n",
      "‚ùå L·ªói hybrid search: 422 Client Error: Unprocessable Entity for url: http://localhost:8002/search\n",
      "üîÑ Fallback to dense search...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "class HybridSearcher:\n",
    "    def __init__(self):\n",
    "        self.embed_url = EMMBED_SERVICE_URL\n",
    "        self.db_url = DATABASE_SERVICE_URL\n",
    "        self.bm25_encoder = BM25Encoder()\n",
    "        self.corpus_fitted = False\n",
    "    \n",
    "    def fit_bm25(self, corpus: List[str]):\n",
    "        \"\"\"Fit BM25 for search\"\"\"\n",
    "        self.bm25_encoder.fit(corpus)\n",
    "        self.corpus_fitted = True\n",
    "    \n",
    "    def embed_query(self, text: str) -> list:\n",
    "        \"\"\"Get dense embedding for query\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.embed_url}/embed\", \n",
    "            json={\"text\": text},\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"embedding\"]\n",
    "    \n",
    "    def create_query_sparse_vector(self, text: str) -> Dict[int, float]:\n",
    "        \"\"\"Create sparse vector for query\"\"\"\n",
    "        if not self.corpus_fitted:\n",
    "            print(\"‚ö†Ô∏è BM25 ch∆∞a ƒë∆∞·ª£c fit. Ch·ªâ s·ª≠ d·ª•ng dense search.\")\n",
    "            return {}\n",
    "        return self.bm25_encoder.encode(text)\n",
    "    \n",
    "    def hybrid_search(self, \n",
    "                     query_text: str, \n",
    "                     limit: int = 5, \n",
    "                     dense_weight: float = 0.7,\n",
    "                     sparse_weight: float = 0.3):\n",
    "        \"\"\"\n",
    "        Hybrid search k·∫øt h·ª£p dense v√† sparse vectors\n",
    "        \n",
    "        Args:\n",
    "            query_text: Text query\n",
    "            limit: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ\n",
    "            dense_weight: Tr·ªçng s·ªë cho semantic search (0-1)\n",
    "            sparse_weight: Tr·ªçng s·ªë cho keyword search (0-1)\n",
    "        \"\"\"\n",
    "        print(f\"üîç Hybrid Search: '{query_text}'\")\n",
    "        print(f\"   üìä Dense weight: {dense_weight}, Sparse weight: {sparse_weight}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. T·∫°o dense vector\n",
    "            dense_vector = self.embed_query(query_text)\n",
    "            \n",
    "            # 2. T·∫°o sparse vector\n",
    "            sparse_vector = self.create_query_sparse_vector(query_text)\n",
    "            \n",
    "            # 3. Chu·∫©n b·ªã query cho Qdrant\n",
    "            if sparse_vector:\n",
    "                # Hybrid search v·ªõi c·∫£ dense v√† sparse\n",
    "                query_data = {\n",
    "                    \"prefetch\": [\n",
    "                        {\n",
    "                            \"query\": dense_vector,\n",
    "                            \"using\": \"dense_vector\",\n",
    "                            \"limit\": limit * 2  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ fusion\n",
    "                        },\n",
    "                        {\n",
    "                            \"query\": {\n",
    "                                \"indices\": list(sparse_vector.keys()),\n",
    "                                \"values\": list(sparse_vector.values())\n",
    "                            },\n",
    "                            \"using\": \"bm25_sparse_vector\", \n",
    "                            \"limit\": limit * 2\n",
    "                        }\n",
    "                    ],\n",
    "                    \"query\": {\n",
    "                        \"fusion\": \"rrf\"  # Reciprocal Rank Fusion\n",
    "                    },\n",
    "                    \"limit\": limit\n",
    "                }\n",
    "            else:\n",
    "                # Ch·ªâ dense search\n",
    "                query_data = {\n",
    "                    \"query\": dense_vector,\n",
    "                    \"using\": \"dense_vector\",\n",
    "                    \"limit\": limit\n",
    "                }\n",
    "            \n",
    "            # 4. G·ª≠i request\n",
    "            response = requests.post(\n",
    "                f\"{self.db_url}/search\",\n",
    "                json=query_data,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            results = response.json()\n",
    "            \n",
    "            # 5. Display results\n",
    "            self.display_hybrid_results(results, query_text)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói hybrid search: {e}\")\n",
    "            # Fallback to simple dense search\n",
    "            return self.fallback_dense_search(query_text, limit)\n",
    "    \n",
    "    def fallback_dense_search(self, query_text: str, limit: int):\n",
    "        \"\"\"Fallback to simple dense search\"\"\"\n",
    "        print(\"üîÑ Fallback to dense search...\")\n",
    "        \n",
    "        dense_vector = self.embed_query(query_text)\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.db_url}/search\",\n",
    "            json={\n",
    "                \"query_vector\": dense_vector,\n",
    "                \"limit\": limit,\n",
    "                \"score_threshold\": 0.5\n",
    "            },\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def display_hybrid_results(self, results, query_text):\n",
    "        \"\"\"Display hybrid search results\"\"\"\n",
    "        if not results or not results.get(\"results\"):\n",
    "            print(\"‚ùå Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüéØ HYBRID SEARCH RESULTS cho: '{query_text}'\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for i, result in enumerate(results[\"results\"]):\n",
    "            print(f\"\\nüî∏ K·∫øt qu·∫£ {i+1}:\")\n",
    "            print(f\"   üìä Hybrid Score: {result['score']:.3f}\")\n",
    "            print(f\"   üìÑ Text: {result['payload']['text'][:200]}...\")\n",
    "            print(f\"   üë§ User: {result['payload'].get('user_id', 'N/A')}\")\n",
    "            print(f\"   üìÅ File: {result['payload'].get('file_id', 'N/A')}\")\n",
    "\n",
    "# S·ª≠ d·ª•ng Hybrid Search\n",
    "searcher = HybridSearcher()\n",
    "\n",
    "# N·∫øu b·∫°n c√≥ corpus ƒë·ªÉ fit BM25\n",
    "# searcher.fit_bm25(data)  # Fit tr√™n corpus ƒë√£ upsert\n",
    "\n",
    "# Test hybrid search\n",
    "test_queries = [\n",
    "    \"Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng\",\n",
    "    \"quy tr√¨nh d·ªçn d·∫πp\",\n",
    "    \"thi·∫øt b·ªã an to√†n\",\n",
    "    \"h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    results = searcher.hybrid_search(\n",
    "        query, \n",
    "        limit=3, \n",
    "        dense_weight=0.7,  # 70% semantic\n",
    "        sparse_weight=0.3  # 30% keyword\n",
    "    )\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c81f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 3 documents v·ªõi Hybrid Search...\n",
      "üîß Fitting BM25 on 3 documents...\n",
      "‚úÖ BM25 fitted successfully\n",
      "üìÑ Processing 1/3: - Gi·ªõi thi·ªáu chung\n",
      "D·ªãch v·ª• D·ªçn d·∫πp Bu·ªìng ph√≤ng c·ªßa...\n",
      "üìÑ Processing 2/3: \n",
      "- H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
      "H∆∞·ªõng d·∫´n ƒë·∫∑t d·ªãch v·ª• d·ªçn d·∫πp...\n",
      "üìÑ Processing 3/3: \n",
      "- L∆∞u √Ω\n",
      "- ƒê∆∞·ª£c ch·ªçn c√πng l√∫c nhi·ªÅu lo·∫°i ph√≤ng, nh...\n",
      "\n",
      "üíæ B·∫Øt ƒë·∫ßu upsert 3 payloads...\n",
      "üîÑ Upsert batch 1: 3 documents...\n",
      "‚úÖ Batch 1 th√†nh c√¥ng (3 documents)\n",
      "\n",
      "üìä Ho√†n th√†nh: 3/3 documents th√†nh c√¥ng\n",
      "üîç Hybrid Search: 'Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng'\n",
      "   üìä Dense weight: 0.7, Sparse weight: 0.3\n",
      "‚ùå L·ªói hybrid search: 422 Client Error: Unprocessable Entity for url: http://localhost:8002/search\n",
      "üîÑ Fallback to dense search...\n"
     ]
    }
   ],
   "source": [
    "# 1. Upsert data v·ªõi hybrid vectors\n",
    "hybrid_pipeline = HybridProcessingPipeline()\n",
    "hybrid_pipeline.process(data)\n",
    "\n",
    "# 2. Search v·ªõi hybrid\n",
    "searcher = HybridSearcher()\n",
    "searcher.fit_bm25(data)  # Fit BM25 tr√™n corpus\n",
    "\n",
    "# 3. Test search\n",
    "results = searcher.hybrid_search(\n",
    "    \"Gi·ªõi thi·ªáu chung v·ªÅ d·ªãch v·ª• d·ªçn d·∫πp bu·ªìng ph√≤ng\",\n",
    "    limit=5,\n",
    "    dense_weight=0.7,   # 70% semantic similarity\n",
    "    sparse_weight=0.3   # 30% keyword matching\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2e053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d742e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
